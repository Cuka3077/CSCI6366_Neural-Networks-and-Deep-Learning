{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Image Classification\n"
      ],
      "metadata": {
        "id": "EfcnAVwyhJXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "BRBd5T4QhDeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Fashion-MNIST train and test CSV files, normalize pixel values, reshape them into image tensors, and build DataLoaders"
      ],
      "metadata": {
        "id": "EoSLv6n-3mGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load dataset & basic preprocessing\n",
        "TRAIN_CSV_PATH = \"/content/fashion-mnist_train.csv\"\n",
        "TEST_CSV_PATH  = \"/content/fashion-mnist_test.csv\"\n",
        "\n",
        "# Load the training and test splits\n",
        "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
        "test_df  = pd.read_csv(TEST_CSV_PATH)\n",
        "\n",
        "def preprocess_fashion_df(df):\n",
        "    # Labels: class ids from 0 to 9\n",
        "    y = df.iloc[:, 0].values\n",
        "\n",
        "    # Pixel data, normalized to [0, 1]\n",
        "    X = df.iloc[:, 1:].values / 255.0\n",
        "\n",
        "    # Reshape to image tensors of shape N x 1 x 28 x 28\n",
        "    X = X.reshape(-1, 1, 28, 28)\n",
        "\n",
        "    X = torch.tensor(X, dtype=torch.float32)\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "    return X, y\n",
        "\n",
        "# Preprocess both train and test splits\n",
        "X_train, y_train = preprocess_fashion_df(train_df)\n",
        "X_test,  y_test  = preprocess_fashion_df(test_df)\n",
        "\n",
        "# Build DataLoaders: shuffle for training, no shuffle for test\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(TensorDataset(X_test,  y_test),  batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "QdpCUD89hagh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Loan Default Prediction"
      ],
      "metadata": {
        "id": "8lab2LKBhwMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch"
      ],
      "metadata": {
        "id": "4I7x6fqUh1t4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load dataset\n",
        "\n",
        "df = pd.read_csv(\"Loan payments data.csv\")\n",
        "\n",
        "# Drop useless ID column\n",
        "df = df.drop(columns=[\"Loan_ID\"])"
      ],
      "metadata": {
        "id": "odCniK_Ei9P2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Binary classification label\n",
        "\n",
        "# Original loan_status may contain:\n",
        "#   PAIDOFF\n",
        "#   COLLECTION\n",
        "#   COLLECTION_PAIDOFF\n",
        "# We define:\n",
        "#   PAIDOFF → 0\n",
        "#   Others  → 1  (default / high-risk)\n",
        "\n",
        "df[\"loan_status_binary\"] = (df[\"loan_status\"] != \"PAIDOFF\").astype(int)"
      ],
      "metadata": {
        "id": "eRdBAziOaOBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Date preprocessing: parse effective_date, due_date, paid_off_time\n",
        "\n",
        "df[\"effective_date\"] = pd.to_datetime(df[\"effective_date\"])\n",
        "df[\"due_date\"] = pd.to_datetime(df[\"due_date\"])\n",
        "df[\"paid_off_time\"] = pd.to_datetime(df[\"paid_off_time\"], errors=\"coerce\")\n",
        "# errors=\"coerce\" converts invalid date strings into NaT"
      ],
      "metadata": {
        "id": "8H3DVmypaSEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Feature engineering from dates\n",
        "\n",
        "# loan duration: due_date - effective_date (days)\n",
        "df[\"loan_duration\"] = (df[\"due_date\"] - df[\"effective_date\"]).dt.days\n",
        "\n",
        "# actual pay duration (NaN if not paid)\n",
        "df[\"actual_pay_duration\"] = (df[\"paid_off_time\"] - df[\"effective_date\"]).dt.days\n",
        "\n",
        "# Missing actual_pay_duration means the loan was NOT fully paid\n",
        "df[\"actual_pay_duration\"] = df[\"actual_pay_duration\"].fillna(0)\n",
        "\n",
        "# Binary indicator whether paid_off_time exists\n",
        "df[\"paid_off_missing\"] = df[\"paid_off_time\"].isna().astype(int)"
      ],
      "metadata": {
        "id": "vk78bKtDaTt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Handle missing values in past_due_days\n",
        "\n",
        "# past_due_days is missing when borrower paid on time:\n",
        "#   NaN → 0 is correct meaning \"no overdue\"\n",
        "\n",
        "df[\"past_due_days\"] = df[\"past_due_days\"].fillna(0)"
      ],
      "metadata": {
        "id": "THay_pohaVnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Select feature columns\n",
        "\n",
        "numeric_cols = [\n",
        "    \"Principal\",\n",
        "    \"terms\",\n",
        "    \"past_due_days\",\n",
        "    \"age\",\n",
        "    \"loan_duration\",\n",
        "    \"actual_pay_duration\",\n",
        "]\n",
        "\n",
        "categorical_cols = [\"education\", \"Gender\"]\n",
        "\n",
        "binary_cols = [\"paid_off_missing\"]  # already numeric & meaningful\n",
        "\n",
        "# Construct feature DataFrame\n",
        "X_raw = df[numeric_cols + categorical_cols + binary_cols].copy()\n",
        "y = df[\"loan_status_binary\"]"
      ],
      "metadata": {
        "id": "6KQRbx5PaXb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. One-hot encode categorical features\n",
        "\n",
        "X_cat = pd.get_dummies(X_raw[categorical_cols], drop_first=True)\n",
        "\n",
        "# Combine:\n",
        "X_processed = pd.concat([\n",
        "    X_raw[numeric_cols],\n",
        "    X_cat,\n",
        "    X_raw[binary_cols]\n",
        "], axis=1)"
      ],
      "metadata": {
        "id": "tS8KgtigaZji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Standardize numerical features\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_processed)"
      ],
      "metadata": {
        "id": "WpCxU2qcabY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train/validation split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "GGG8IBlvac3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Convert to PyTorch tensors\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val.values, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "9v72IC6naeY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Compute class weights for imbalanced classification\n",
        "\n",
        "num_pos = (y_train == 1).sum().item()\n",
        "num_neg = (y_train == 0).sum().item()\n",
        "pos_weight_value = num_neg / num_pos  # > 1 → positive class rarer\n",
        "\n",
        "pos_weight = torch.tensor([pos_weight_value], dtype=torch.float32)\n",
        "\n",
        "print(\"Class counts:\", num_neg, \"(no default) /\", num_pos, \"(default)\")\n",
        "print(\"pos_weight for BCEWithLogitsLoss =\", pos_weight.item())\n",
        "\n",
        "# Now you can pass pos_weight into BCEWithLogitsLoss:\n",
        "# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "print(\"Preprocessing complete. X_train, X_val, y_train, y_val are ready.\")\n",
        "print(\"Input dim =\", X_train.shape[1])"
      ],
      "metadata": {
        "id": "SA33TVIgaful"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}