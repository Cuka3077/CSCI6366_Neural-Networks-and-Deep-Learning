# CSCI6366_Neural-Networks-and-Deep-Learning
Comparative Analysis of Activation Functions in Deep Learning

Team Members: Sapu Du G36715715   Zhitao Liu G48328246   Lareina Cao G27616034

ðŸ“– Project Overview
This final project conducts a comprehensive comparative analysis of several fundamental and modern activation functions in deep learning. Activation functions are crucial components that introduce non-linearity into neural networks, enabling them to learn complex patterns.

We systematically evaluate the performance of the following activation functions across different model architectures and hyperparameter settings:

Linear, Sigmoid, ReLU, Leaky ReLU.

The primary goal is to provide practical, empirical insights into how these functions influence training dynamics, convergence speed, and final model performance. The results of this survey aim to serve as a guide for selecting or designing activation functions to improve the efficiency and robustness of deep neural networks.

ðŸŽ¯ Key Objectives
To implement and benchmark a diverse set of activation functions.

To analyze their performance across different model architectures (e.g., MLP, CNNs).

To investigate their behavior under various hyperparameter combinations (e.g., learning rates, optimizers).

To compare key metrics such as training/validation accuracy, loss convergence, and model robustness.
