# ðŸ“Œ**CSCI6366_Neural-Networks-and-Deep-Learning**
Comparative Analysis of Activation Functions in Deep Learning

## ðŸ‘¥Team Members: 
*Sapu Du* G36715715, GitHub username: Cuka3077  
*Zhitao Liu* G48328246, GitHub username: liutian388-netizen  
*Lareina Cao* G27616034, GitHub username: Lareina-c  

## ðŸ“– Project Overview
This project investigates how different activation functions affect model performance across two distinct machine learning tasks: image classification and loan default prediction.
We systematically evaluate four activation functions â€” Linear, Sigmoid, ReLU, and Leaky ReLU â€” under varying hyperparameter settings (learning rate, initialization, etc.).

The project consists of two tasks:

**Task 1: Image Classification**  
Model: Convolutional Neural Network (CNN)  
Dataset: Fashion-MNIST  
Goal: Assess activation function performance in a complex, high-dimensional vision task.

**Task 2: Loan Default Prediction**  
Model: Multi-Layer Perceptron (MLP)  
Dataset: Loan data  
Goal: Compare activation functions in a structured data prediction problem.

Through experiments on these two tasks, we aim to understand how activation functions behave across different data modalities and model architectures.


## ðŸ“‚ Datasets and Sources
**Fashion-MNIST**  
Source: https://www.kaggle.com/datasets/zalando-research/fashionmnist?resource=download

**Loan Data**  
Source: https://www.kaggle.com/datasets/zhijinzhai/loandata

## ðŸŽ¯ Key Objectives
To implement and benchmark a diverse set of activation functions.

To analyze their performance across different model architectures (e.g., MLP, CNNs).

To investigate their behavior under various hyperparameter combinations (e.g., learning rates, optimizers).

To compare key metrics such as training timeï¼Œtraining/validation accuracy, loss convergence, and model robustness.
